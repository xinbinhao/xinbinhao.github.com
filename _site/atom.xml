<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>xinbinhao</title>
 <link href="http://xinbinhao.github.io/" rel="self"/>
 <link href="http://xinbinhao.github.io"/>
 <updated>2014-10-13T18:35:25+08:00</updated>
 <id>http://xinbinhao.github.io</id>
 <author>
   <name>xinbinhao</name>
   <email>haoxinbin@gmail.com</email>
 </author>

 
 <entry>
   <title>全局唯一ID生成算法</title>
   <link href="http://xinbinhao.github.io/2014/09/22/idcreater"/>
   <updated>2014-09-22T00:00:00+08:00</updated>
   <id>http://xinbinhao.github.io/2014/09/22/idcreater</id>
   <content type="html">&lt;p&gt;在分布式系统中经常遇到需要获取唯一ID的情况，在保证唯一的前提下一定要高性能，并且要相对占用很少的空间。&lt;/p&gt;

&lt;p&gt;下面主要介绍几种生成方法：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;一、数据库方式&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1.1、单库单表方式&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这个是很简单的方式了，用一个库一个表存放所有数据，数据库根据+1的方式获取。&lt;/p&gt;

&lt;p&gt;优点：简单&lt;/p&gt;

&lt;p&gt;缺点：大并发时性能较差&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1.2、数据库+分表设计&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;按照一定的区域把数据划分到不同的表，分布式中机器按照一定的方式到对应的表获取。&lt;/p&gt;

&lt;p&gt;比如说有 256个表，每个表放得是10w的数据，(第一个表0-10w,第二个表100001 - 20w，后面类似)，分布式中机器按照一定的规则 %256 到那个表就从对应表获取一个数字。当一个表数据用完后在重新分配。&lt;/p&gt;

&lt;p&gt;优点：相比1.1方式性能有了一定的提升&lt;/p&gt;

&lt;p&gt;缺点：需要提前分配好数据区域，当达到一定的并发后还会遇到性能问题。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;二、语言自带方式&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;很多语言都自带了方法。比如java 中UUID方式 UUID.randomUUID().toString()&lt;/p&gt;

&lt;p&gt;```javascript&lt;/p&gt;

&lt;p&gt;/**
* get Id by UUID
* @return
*/
public static String createId(){
     return UUID.randomUUID().toString();
}&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;三、本机ip+当时时间+累加因子 方式&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;```javascript&lt;/p&gt;

&lt;p&gt;public class IdGenerator {&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static AtomicInteger count = new AtomicInteger(1024);

/**
* get Id by localIP + timeStamp + count
* @return
*/
public static String createIdByLocalIp() {
    return getId(InetAddressUtil.getLocalIpHex(), System.currentTimeMillis(), getNextId());
}

/**
* get Id by IP + timeStamp + count
* @param ip input IP
* @return
*/
public static String createIdByIp(String ip) {
    if ((ip != null) &amp;amp;&amp;amp; (!(ip.isEmpty())) &amp;amp;&amp;amp; (InetAddressUtil.validate(ip))){
        return getId(InetAddressUtil.IpToHex(ip), System.currentTimeMillis(),getNextId());
    }
    return createIdByLocalIp();
} 

private static String getId(String ip, long timestamp, int nextId) {
    StringBuilder appender = new StringBuilder(25);
    appender.append(ip).append(timestamp).append(nextId);
    return appender.toString();
}

private static int getNextId() {
    while (true) {
        int current = count.get();
        int next = (current &amp;gt; 8192) ? 1024 : current + 1;
        if (count.compareAndSet(current, next))
            return next;
    }
} }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;p&gt;详细代码请查看github项目。&lt;/p&gt;

&lt;p&gt;该方式在一台机器如果启一个进程，那是可以保证唯一性的，如果一个机器启多个进程有可能会重复，因为ip是一样的，当前时间和后面的累加因子也完全是有概率重复的。由于该方式返回内容是字符串，在需要拿当前id做数据库主键存储时可能会占用空间比较大。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;四、时间+机器id+业务id+累加因子 方式&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;该方式返回long类型，可以根据不同的进程，不同的业务做处理。&lt;/p&gt;

&lt;p&gt;long类型：64位ID (42(毫秒)+5(机器ID)+5(业务编码)+12(重复累加因子))&lt;/p&gt;

&lt;p&gt;12位重复累加因子是为了防止多线程获取时前面52位生成id一致而做的处理。&lt;/p&gt;

&lt;p&gt;下面代码类似twitter id 生成算法。&lt;/p&gt;

&lt;p&gt;```javascripet&lt;/p&gt;

&lt;p&gt;/**
 * 唯一ID 生成器
 * 64位ID (42(毫秒)+5(机器ID)+5(业务编码)+12(重复累加))
 */
public class IdCreater {
	private final static long idepoch = 1288834974657L;
	// 机器标识位数
	private final static long workerIdBits = 5L;
	// 业务标识位数
	private final static long datacenterIdBits = 5L;
	// 机器ID最大值
	private final static long maxWorkerId = -1L ^ (-1L « workerIdBits);
	// 业务ID最大值
	private final static long maxDatacenterId = -1L ^ (-1L « datacenterIdBits);
	// 毫秒内自增位
	private final static long sequenceBits = 12L;
	// 机器ID偏左移12位
	private final static long workerIdShift = sequenceBits;
	// 业务ID左移17位
	private final static long datacenterIdShift = sequenceBits + workerIdBits;
	// 时间毫秒左移22位
	private final static long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private final static long sequenceMask = -1L ^ (-1L &amp;lt;&amp;lt; sequenceBits);

private static long lastTimestamp = -1L;

private long sequence = 0L;
private final long workerId;
private final long datacenterId;

public IdCreater(long workerId, long datacenterId) {
	if (workerId &amp;gt; maxWorkerId || workerId &amp;lt; 0) {
		throw new IllegalArgumentException(&quot;worker Id can&#39;t be greater than %d or less than 0&quot;);
	}
	if (datacenterId &amp;gt; maxDatacenterId || datacenterId &amp;lt; 0) {
		throw new IllegalArgumentException(&quot;datacenter Id can&#39;t be greater than %d or less than 0&quot;);
	}
	this.workerId = workerId;
	this.datacenterId = datacenterId;
}

public IdCreater(long workerId) {
	if (workerId &amp;gt; maxWorkerId || workerId &amp;lt; 0) {
		throw new IllegalArgumentException(&quot;worker Id can&#39;t be greater than %d or less than 0&quot;);
	}
	this.workerId = workerId;
	this.datacenterId = 0;
}

public long generate(){
	return this.nextId(false, 0);
}

public long generate(long busid){
	return this.nextId(true, busid);
}

private synchronized long nextId(boolean isPadding, long busid) {
	long timestamp = timeGen();
	long paddingnum = datacenterId;
	if(isPadding){
		paddingnum = busid;
	}
	if (timestamp &amp;lt; lastTimestamp) {
		try {
			throw new Exception(&quot;Clock moved backwards.  Refusing to generate id for &quot;+ (lastTimestamp - timestamp) + &quot; milliseconds&quot;);
		} catch (Exception e) {
			e.printStackTrace();
		}
	}

	if (lastTimestamp == timestamp) {
		sequence = (sequence + 1) &amp;amp; sequenceMask;
		if (sequence == 0) {
			timestamp = tailNextMillis(lastTimestamp);
		}
	} else {
		sequence = 0;
	}
	lastTimestamp = timestamp;
	long nextId = ((timestamp - idepoch) &amp;lt;&amp;lt; timestampLeftShift)
			| (paddingnum &amp;lt;&amp;lt; datacenterIdShift)
			| (workerId &amp;lt;&amp;lt; workerIdShift) | sequence;

	return nextId;
}

private long tailNextMillis(final long lastTimestamp) {
	long timestamp = this.timeGen();
	while (timestamp &amp;lt;= lastTimestamp) {
		timestamp = this.timeGen();
	}
	return timestamp;
}

private long timeGen() {
	return System.currentTimeMillis();
} }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;总结：&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;如果在小一点的系统，并发很低的情况，采用数据库的方式已经足够了。如果对性能有要求只需要一个唯一id，java的uuid也是一种很好的方式，在一台物理机不需要根据业务和进程进行区分id时，建议采用第三种方式。第四种方式可以根据不同的进程和业务编码生成对应的long类型的id，可以在单台物理机部署很多id生成器，但机器和业务id会有位数限制（比如机器id是5位表示，那么最多的排列是5位的组合，业务编码也一样）。在分布式系统中可以创建几个Id生成服务，分别部署在不同的机器，这种既可以解决机器id和业务id长度的限制，也可以高效的生成id。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;其实在很多业务场景，生成Id的规则都可以灵活变化，针对分库分表的业务，生成的Id中可以包含库id，比如：业务编码变成库id，但要考虑将来db扩容，id生成器中位数不够的情况。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

</content>
 </entry>
 
 <entry>
   <title>TCP系列一:宕机</title>
   <link href="http://xinbinhao.github.io/2014/08/25/tcp-poweroff"/>
   <updated>2014-08-25T00:00:00+08:00</updated>
   <id>http://xinbinhao.github.io/2014/08/25/tcp-poweroff</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;场景&lt;/strong&gt;：
客户端与服务端通过tcp长连接的方式进行数据交互，当服务端机器突然宕机(掉电、拔网线等)客户端会发生什么。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;过程&lt;/strong&gt;：
&amp;gt; 正常情况下当服务端执行kill -9/-12/-15 客户端会收到-1的数据内容，当客户端收到-1数据信息时，执行关闭socket既可以正常关闭socket。（关闭socket执行4次握手过程）&lt;/p&gt;

&lt;p&gt;回到问题本身，为了复现该问题，我们找两台机器进行测试，客户端(x.x.156.57)服务端(x.x.36.14)通过tcp长连接进行数据交互，客户端先发送10条数据到服务端，然后sleep 3分钟，客户端sleep的过程中，关闭服务端(服务端机器不关进程直接执行poweroff模拟宕机情况).客户端sleep 3分钟后，在给服务端发送一条数据。&lt;/p&gt;

&lt;p&gt;1、当服务端宕机，在客户端机器执行命令：netstat/ss |grep 服务端端口
会发现当前的tcp连接还是正常状态(为什么？因为tcp长连接正常关闭需要4次握手过程，而针对服务端突然宕机情况是没有正常执行4次握手过程)&lt;/p&gt;

&lt;blockquote&gt;
  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;**ss -na&lt;/td&gt;
        &lt;td&gt;grep x.x.36.14**&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;ESTAB      0      0       ::ffff:x.x.156.57:42218   ::ffff:x.x.36.14:16019 &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;2、服务端关机后一直不重启，客户端过一段时间后会自动断开连接(为什么？时间是否可以控制？)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;tcpdump&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;```javascript&lt;/p&gt;

&lt;p&gt;17:31:16.236188 IP (tos 0x0, ttl 63, id 38272, offset 0, flags [DF], proto TCP (6), length 142)
x.x.36.14.16019 &amp;gt; x.x.156.57.42215: Flags [P.], cksum 0xcf84 (correct), seq 721:811, ack 919, win 114, options [nop,nop,TS val 665838 ecr 27489167], length 90&lt;/p&gt;

&lt;p&gt;17:31:16.275492 IP (tos 0x0, ttl 64, id 41102, offset 0, flags [DF], proto TCP (6), length 52)
x.x.156.57.42215 &amp;gt; x.x.36.14.16019: Flags [.], cksum 0xf944 (correct), ack 811, win 115, options [nop,nop,TS val 27489209 ecr 665838], length 0&lt;/p&gt;

&lt;p&gt;17:34:16.239767 IP (tos 0x0, ttl 64, id 41103, offset 0, flags [DF], proto TCP (6), length 154)
x.x.156.57.42215 &amp;gt; x.x.36.14.16019: Flags [P.], cksum 0xd547 (incorrect -&amp;gt; 0x51f7), seq 919:1021, ack 811, win 115, options [nop,nop,TS val 27669172 ecr 665838], length 102&lt;/p&gt;

&lt;p&gt;17:34:16.440506 IP (tos 0x0, ttl 64, id 41104, offset 0, flags [DF], proto TCP (6), length 154)
x.x.156.57.42215 &amp;gt; x.x.36.14.16019: Flags [P.], cksum 0xd547 (incorrect -&amp;gt; 0x512d), seq 919:1021, ack 811, win 115, options [nop,nop,TS val 27669374 ecr 665838], length 102&lt;/p&gt;

&lt;p&gt;17:34:16.844514 IP (tos 0x0, ttl 64, id 41105, offset 0, flags [DF], proto TCP (6), length 154)
x.x.156.57.42215 &amp;gt; x.x.36.14.16019: Flags [P.], cksum 0xd547 (incorrect -&amp;gt; 0x4f99), seq 919:1021, ack 811, win 115, options [nop,nop,TS val 27669778 ecr 665838], length 102&lt;/p&gt;

&lt;p&gt;17:34:17.652631 IP (tos 0x0, ttl 64, id 41106, offset 0, flags [DF], proto TCP (6), length 154)
    x.x.156.57.42215 &amp;gt; x.x.36.14.16019: Flags [P.], cksum 0xd547 (incorrect -&amp;gt; 0x4c71), seq 919:1021, ack 811, win 115, options [nop,nop,TS val 27670586 ecr 665838], length 102&lt;/p&gt;

&lt;p&gt;17:34:19.268528 IP (tos 0x0, ttl 64, id 41107, offset 0, flags [DF], proto TCP (6), length 154)
    x.x.156.57.42215 &amp;gt; x.x.36.14.16019: Flags [P.], cksum 0xd547 (incorrect -&amp;gt; 0x4621), seq 919:1021, ack 811, win 115, options [nop,nop,TS val 27672202 ecr 665838], length 102&lt;/p&gt;

&lt;p&gt;17:34:22.500519 IP (tos 0x0, ttl 64, id 41108, offset 0, flags [DF], proto TCP (6), length 154)
x.x.156.57.42215 &amp;gt; x.x.36.14.16019: Flags [P.], cksum 0xd547 (incorrect -&amp;gt; 0x3981), seq 919:1021, ack 811, win 115, options [nop,nop,TS val 27675434 ecr 665838], length 102&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;第1行正常数据发送，第2行返回ack，第三行发送数据，后面5行数据重发。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从tcp底层看当服务端宕机后，客户端看当前连接是正常的，所以客户端还会通过该连接进行数据交互(但其实是传递不到服务端的)，如果发送的数据量很大可以通过
netstat/ss 中得send-q 参数看出(send-q会很大，这里还有个问题如果服务端没有宕，tcp连接也正常，但send-q堆积很大时会发生什么？这个通过另一文章阐述)，&lt;/p&gt;

&lt;p&gt;从tcpdump看客户端发送数据后没有返回ack，客户端会重发，重发5次后断开了连接(为什么是5次？)。
这个就要从操作系统tcp参数说起了。&lt;/p&gt;

&lt;p&gt;Centos (不同的系统对应参数也不同)
more /proc/sys/net/ipv4/tcp_retries2(默认15) &lt;/p&gt;

&lt;p&gt;tcp_retries2 ：INTEGER
默认值为15
在丢弃激活(已建立通讯状况)的TCP连接之前﹐需要进行多少次重试。默认值为15，根据RTO的值来决定，相当于13-30分钟(RFC1122规定，必须大于100秒).(这个值根据目前的网络设置,可以适当地改小,我的网络内修改为了5)&lt;/p&gt;

&lt;p&gt;所以为什么会是重发5次后客户端主动断开连接，客户端主动断开连接其实是会发一个rst的。&lt;/p&gt;

&lt;p&gt;ps &lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;至于重发时间间隔可以参考下 &lt;strong&gt;tcp/ip卷 1&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

</content>
 </entry>
 
 
</feed>
